<div align="center"><h1>Air Quality Data Pipeline</h1></div>

<p>
Pipeline completo para ingest√£o, processamento, cache, armazenamento, gera√ß√£o de alertas e visualiza√ß√£o de dados de qualidade do ar, utilizando arquitetura distribu√≠da com Kafka, Redis, RabbitMQ, MinIO, Flask, Celery, Airflow e Streamlit.
</p>

<div align="center"><h2>üêç Ferramentas e Linguagens Utilizadas üêç</h2></div>

<div align="center">
  <img src="https://skillicons.dev/icons?i=python,docker,flask,kafka,redis,rabbitmq" /><br>
  <img src="https://skillicons.dev/icons?i=minio,celery" />
</div>

<div align="center"><h2>Resumo</h2></div>

<p>
Este projeto implementa um pipeline de dados orientado a eventos para o monitoramento da qualidade do ar. Ele cobre desde a ingest√£o via Kafka at√© a visualiza√ß√£o em tempo real via Streamlit, passando por processamento ass√≠ncrono com Celery, armazenamento em cache (Redis), arquivamento (MinIO) e alertas (RabbitMQ). Tudo √© orquestrado por DAGs no Airflow.
</p>

<div align="center"><h2>Descri√ß√£o</h2></div>

<p>
Este projeto implementa uma arquitetura robusta e escal√°vel para ingest√£o e an√°lise de dados ambientais. O fluxo come√ßa com um Producer lendo dados de um dataset CSV e enviando-os ao Kafka. Um Worker com Celery consome e processa os dados, armazenando resultados em Redis (cache) e MinIO (data lake), al√©m de emitir alertas cr√≠ticos para o RabbitMQ. Uma API Flask unifica o acesso aos dados e o Airflow orquestra todo o pipeline. Por fim, o Streamlit oferece um dashboard em tempo real com as leituras dos sensores. Ideal para projetos de IoT, dados em tempo real e visualiza√ß√£o anal√≠tica.
</p>

<div align="center"><h2>O que foi desenvolvido</h2></div>

- Leitor de dados (Producer Kafka)
- Consumidor com processamento ass√≠ncrono (Celery)
- Armazenamento em Redis (cache) e MinIO (hist√≥rico)
- Gera√ß√£o de alertas com RabbitMQ
- API REST com Flask
- Orquestra√ß√£o de tarefas com Airflow
- Dashboard em tempo real com Streamlit

<div align="center"><h2>Estrutura do Projeto</h2></div>
