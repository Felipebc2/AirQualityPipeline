<div align="center"><h1>Air Quality Data Pipeline</h1></div>

<p>
Pipeline completo para ingestÃ£o, processamento, cache, armazenamento, geraÃ§Ã£o de alertas e visualizaÃ§Ã£o de dados de qualidade do ar, utilizando arquitetura distribuÃ­da com Kafka, Redis, RabbitMQ, MinIO, Flask, Celery, Airflow e Streamlit.
</p>

<div align="center"><h2>ğŸ Ferramentas e Linguagens Utilizadas ğŸ</h2></div>

<div align="center">
  <img src="https://skillicons.dev/icons?i=python,docker,flask,kafka,redis,rabbitmq" /><br>
  <img src="https://skillicons.dev/icons?i=minio,celery" />
</div>

<div align="center"><h2>Resumo</h2></div>

<p>
Este projeto implementa um pipeline de dados orientado a eventos para o monitoramento da qualidade do ar. Ele cobre desde a ingestÃ£o via Kafka atÃ© a visualizaÃ§Ã£o em tempo real via Streamlit, passando por processamento assÃ­ncrono com Celery, armazenamento em cache (Redis), arquivamento (MinIO) e alertas (RabbitMQ). Tudo Ã© orquestrado por DAGs no Airflow.
</p>

<div align="center"><h2>DescriÃ§Ã£o</h2></div>

<p>
Este projeto implementa uma arquitetura robusta e escalÃ¡vel para ingestÃ£o e anÃ¡lise de dados ambientais. O fluxo comeÃ§a com um Producer lendo dados de um dataset CSV e enviando-os ao Kafka. Um Worker com Celery consome e processa os dados, armazenando resultados em Redis (cache) e MinIO (data lake), alÃ©m de emitir alertas crÃ­ticos para o RabbitMQ. Uma API Flask unifica o acesso aos dados e o Airflow orquestra todo o pipeline. Por fim, o Streamlit oferece um dashboard em tempo real com as leituras dos sensores. Ideal para projetos de IoT, dados em tempo real e visualizaÃ§Ã£o analÃ­tica.
</p>

<div align="center">
  <img src="streaming_sim.png" alt="Estrutura do Pipeline">
</div>

<div align="center"><h2>O que foi desenvolvido</h2></div>

- Leitor de dados (Producer Kafka)
- Consumidor com processamento assÃ­ncrono (Celery)
- Armazenamento em Redis (cache) e MinIO (histÃ³rico)
- GeraÃ§Ã£o de alertas com RabbitMQ
- API REST com Flask
- OrquestraÃ§Ã£o de tarefas com Airflow
- Dashboard em tempo real com Streamlit

<div align="center"><h2>Estrutura do Projeto</h2></div>

```
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ streaming_sim.png
â”œâ”€â”€ pipeline/
â”‚    â”œâ”€â”€ pipeline/
â”‚    â”‚   â”œâ”€â”€ __init__.py
â”‚    â”‚   â”œâ”€â”€ alert_handler.py
â”‚    â”‚   â”œâ”€â”€ config.py
â”‚    â”‚   â”œâ”€â”€ consumer.py
â”‚    â”‚   â”œâ”€â”€ pipeline.py
â”‚    â”‚   â”œâ”€â”€ producer.py
â”‚    â”‚   â”œâ”€â”€ storage.py
â”‚    â”‚   â””â”€â”€ utils.py
â”‚    â”œâ”€â”€ streamlit_app/
â”‚    â”‚   â””â”€â”€ app.py
â”‚    â”œâ”€â”€ Dockerfile
â”‚    â”œâ”€â”€ README.md
â”‚    â”œâ”€â”€ airquality.csv
â”‚    â”œâ”€â”€ docker-compose.yml
â”‚    â”œâ”€â”€ test.py
â”‚    â”œâ”€â”€ requirements.txt
```

<div align="center"><h2>ExecuÃ§Ã£o do Projeto</h2></div>

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/sarapacheco-user/AirQualityPipeline.git
cd AirQualityPipeline

# 2. Atualize dependÃªncias do sistema
sudo apt-get update && sudo apt-get install -y libjpeg-dev zlib1g-dev

# 3. Crie e ative um ambiente virtual
python3 -m venv venv
source venv/bin/activate

# 4. Limpe e instale os pacotes do projeto
pip cache purge
pip install --no-cache-dir -r requirements.txt

# 5. Suba os containers com Docker Compose
docker-compose up -d --build

# 6. Execute o Streamlit
streamlit run streamlit_app/app.py
